{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyML/nYabpFXx0pAN0g1XLBe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakshitha2207/Vector-store---Embedding-model/blob/main/vector_store.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required libraries"
      ],
      "metadata": {
        "id": "yTTB9sL4g366"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-XoXfOaPeSy",
        "outputId": "ca7e3843-403a-44ba-d69b-e9a199d93c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/164.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/718.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m716.8/718.3 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/294.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U \\\n",
        "     Sentence-transformers==3.0.1 \\\n",
        "     langchain==0.2.11 \\\n",
        "     langchain-huggingface==0.0.3 \\\n",
        "     langchain-google-genai==1.0.7 \\\n",
        "     langchain-chroma==0.1.2 \\\n",
        "     langchain-community==0.2.10 \\\n",
        "     einops==0.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required libraries"
      ],
      "metadata": {
        "id": "HJqLNUR9hDyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from langchain_google_genai import (\n",
        "    ChatGoogleGenerativeAI,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        ")\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "kNcmOYm4PsGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Provide Google API Key.\n",
        "\n",
        "It can be used both for Gemini Pro LLM  & Google Embedding Model. You can create Google API key using following link\n",
        "\n",
        "1. [Google Gemini-Pro API Key](https://console.cloud.google.com/apis/credentials)\n",
        "\n",
        "2. [YouTube Video explaining Google API Key](https://www.youtube.com/watch?v=ZHX7zxvDfoc)\n",
        "\n"
      ],
      "metadata": {
        "id": "a8d_ukT8hNdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR8-wWovTvZ7",
        "outputId": "749dc65d-6d45-434e-f18d-beab30229264"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Provide Huggingface API Key.\n",
        "\n",
        "If you want to use Huggingface Embedding Models. You can create Huggingface API key using following link\n",
        "\n",
        "1. [Huggingface API Key](https://huggingface.co/settings/tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ktS1Nl9RhPlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwSTwMRNP44T",
        "outputId": "8a18ceb8-fc3c-4680-b904-b928bc1ecce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Huggingface based Nomic AI Embedding model\n",
        "\n",
        "You can use any other huggingface open-source embedding models as per your requirement, fitness and system constraints. You can get the model name from MTEB leaderboard.\n",
        "\n",
        "Popular Models\n",
        "\n",
        "1. nomic-ai/nomic-embed-text-v1.5\n",
        "2. nomic-ai/nomic-embed-text-v1\n",
        "3. sentence-transformers/all-MiniLM-L12-v2\n",
        "4. sentence-transformers/all-MiniLM-L6-v2\n",
        "\n",
        ".....\n",
        "\n"
      ],
      "metadata": {
        "id": "Hxmb6c-ThkL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "def get_huggingface_embeddings():\n",
        "\n",
        "    # Change model_name as per your choosen huggingface embedding model\n",
        "    nomic_embeddings = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1.5\", model_kwargs = {'trust_remote_code': True})\n",
        "    return nomic_embeddings"
      ],
      "metadata": {
        "id": "SihjANeZP5f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's implement Basic RAG using following components\n",
        "\n",
        "1. **Chroma:** Used as the vector store for efficient similarity search.\n",
        "2. **Embedding Models:** As we choose OpenAI, Google or Huggingface Embedding Models\n",
        "3. **ChatGoogleGenerativeAI:** The Gemini Pro model used for generating responses.\n",
        "4. **cosine_similarity:** Used for computing similarities in the evaluation step."
      ],
      "metadata": {
        "id": "RSFBSyEDh5UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for printing docs\n",
        "def pretty_print_docs(docs):\n",
        "    # Iterate through each document and format the output\n",
        "    for i, d in enumerate(docs):\n",
        "        print(f\"{'-' * 50}\\nDocument {i + 1}:\")\n",
        "        print(f\"Content:\\n{d.page_content}\\n\")\n",
        "        print(\"Metadata:\")\n",
        "        for key, value in d.metadata.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "    print(f\"{'-' * 50}\")  # Final separator for clarity\n",
        "\n",
        "# Example usage\n",
        "# Assuming `docs` is a list of Document objects"
      ],
      "metadata": {
        "id": "lt-yjbZMP-De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Load and preprocess data code"
      ],
      "metadata": {
        "id": "z1PQN1Enh_Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_data(url):\n",
        "    # Load data from web\n",
        "    loader = WebBaseLoader(url)\n",
        "    data = loader.load()\n",
        "\n",
        "    # Split text into chunks (Experiment with Chunk Size and Chunk Overlap to get optimal chunking)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_documents(data)\n",
        "\n",
        "    # Add unique IDs to each text chunk\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        chunk.metadata[\"id\"] = idx\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "LI3ZhmT9QEyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Intialize Embedding Model"
      ],
      "metadata": {
        "id": "s2nI9Lu-iFPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_model():\n",
        "        embeddings = get_huggingface_embeddings()\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "FudRYeP3QIPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Create Chroma vector store"
      ],
      "metadata": {
        "id": "Fb0MTNFxiMLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_stores(chunks, embeddings):\n",
        "    # Create vector stores using the specified embedding model\n",
        "    vectorstore = Chroma.from_documents(chunks, embeddings)\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "nzxHsSULQKad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Implement Basic RAG\n",
        "\n",
        "1. **Retrieval:** We use the Chroma vector store's similarity search to retrieve the top 5 most relevant documents for the query.\n",
        "2. **Context Formation:** We combine the retrieved documents into a single context string.\n",
        "3. **Response Generation:** Using the Gemini Pro model, we generate a final response based on the retrieved context and the query."
      ],
      "metadata": {
        "id": "GoY29pDziP2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_rag(query, vectorstore, llm):\n",
        "    # Retrieve relevant documents\n",
        "    docs = vectorstore.similarity_search(query, k=5)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Generate response\n",
        "    prompt = f\"{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"final_answer\": response.content,\n",
        "        \"retrieval_method\": \"Basic similarity search\",\n",
        "        \"context\": context\n",
        "    }"
      ],
      "metadata": {
        "id": "uQ6sDcVzQMiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: RAG Evaluation\n",
        "\n",
        "1. We compute embeddings for the query, response, and context.\n",
        "2. We calculate cosine similarities between query-response and response-context.\n",
        "3. We compute an overall relevance score as the average of these similarities.\n"
      ],
      "metadata": {
        "id": "rTIAugoBiTu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_response(query, embeddings, response, context):\n",
        "    # Compute embeddings\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "    response_embedding = embeddings.embed_query(response)\n",
        "    context_embedding = embeddings.embed_query(context)\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    query_response_similarity = cosine_similarity([query_embedding], [response_embedding])[0][0]\n",
        "    response_context_similarity = cosine_similarity([response_embedding], [context_embedding])[0][0]\n",
        "\n",
        "    # Compute relevance score (average of the two similarities)\n",
        "    relevance_score = (query_response_similarity + response_context_similarity) / 2\n",
        "\n",
        "    return {\n",
        "        \"query_response_similarity\": query_response_similarity,\n",
        "        \"response_context_similarity\": response_context_similarity,\n",
        "        \"relevance_score\": relevance_score\n",
        "    }"
      ],
      "metadata": {
        "id": "wf_1WJ_SQPro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Load and process data and create chunks to store in the Chroma Vector Store\n",
        "\n",
        "1. [Langchain Chunking Strategy](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)\n",
        "2. [Langchain Vectorstore](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/)\n"
      ],
      "metadata": {
        "id": "PnOmTxIiiaP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the gemini-pro language model with specified settings (Change temeprature  and other parameters as per your requirement)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, safety_settings={\n",
        "          HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        },)\n",
        "\n",
        "# Load Documents from a web URL\n",
        "url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
        "chunks = load_and_process_data(url)\n",
        "\n",
        "# Get Embedding Model.\n",
        "# embedding_model value options (openai, google, huggingface)\n",
        "embeddings = get_embedding_model()\n",
        "\n",
        "# Create vector stores using the specified embedding model\n",
        "vectorstore = create_vector_stores(chunks, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjMqp5DjQTIx",
        "outputId": "2912fc87-5a84-40d5-cb39-8a9720bc5e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.eb02ceb48c1fdcc477ff1925c9732c379f0f0d1f.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: RAG in Action to test Embedding Model\n",
        "\n",
        "## This implementation demonstrashowstes the key parts of Basic RAG with evaluation:\n",
        "\n",
        "1. Chunking and embedding of source documents\n",
        "2. Retrieval of relevant documents based on query similarity\n",
        "3. Generation of a response using the retrieved context\n",
        "4. Evaluation of the response based on its similarity to both the query and the context"
      ],
      "metadata": {
        "id": "E1vz9q51ifHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the main applications of artificial intelligence in healthcare?\"\n",
        "result = basic_rag(query, vectorstore, llm)\n",
        "\n",
        "\n",
        "print(f\"Query: {result['query']}\")\n",
        "print(\"=========================\")\n",
        "print(f\"Final Answer: {result['final_answer']}\")\n",
        "print(\"=========================\")\n",
        "print(f\"Retrieval Method: {result['retrieval_method']}\")\n",
        "\n",
        "\n",
        "# Evaluate the response\n",
        "evaluation = evaluate_response(query, embeddings, result[\"final_answer\"], result[\"context\"])\n",
        "print(\"\\nEvaluation:\")\n",
        "print(f\"Query-Response Similarity: {evaluation['query_response_similarity']:.4f}\")\n",
        "print(f\"Response-Context Similarity: {evaluation['response_context_similarity']:.4f}\")\n",
        "print(f\"Overall Relevance Score: {evaluation['relevance_score']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xbh4Pv4yQWYU",
        "outputId": "34250dff-8369-44a1-b0f4-bed195ca3423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What are the main applications of artificial intelligence in healthcare?\n",
            "=========================\n",
            "Final Answer: The main applications of artificial intelligence in healthcare include increasing patient care and quality of life, more accurately diagnosing and treating patients, processing and integrating big data, and deepening the understanding of biomedically relevant pathways.\n",
            "=========================\n",
            "Retrieval Method: Basic similarity search\n",
            "\n",
            "Evaluation:\n",
            "Query-Response Similarity: 0.9467\n",
            "Response-Context Similarity: 0.8284\n",
            "Overall Relevance Score: 0.8876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Demonstration:\n",
        "\n",
        "We showcase the retrieval process by displaying the retrieved documents for a sample query."
      ],
      "metadata": {
        "id": "zJB2x0krijuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate retrieval\n",
        "demo_query = \"Explain the concept of machine learning and its relationship to AI\"\n",
        "print(f\"\\nDemonstration Query: {demo_query}\")\n",
        "\n",
        "# Retrieve documents\n",
        "docs = vectorstore.similarity_search(demo_query, k=5)\n",
        "print(\"\\nRetrieved Documents:\")\n",
        "pretty_print_docs(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW2CXLVaQZad",
        "outputId": "d84c1d4a-08e2-4070-e82b-bc223cf3e432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Demonstration Query: Explain the concept of machine learning and its relationship to AI\n",
            "\n",
            "Retrieved Documents:\n",
            "--------------------------------------------------\n",
            "Document 1:\n",
            "Content:\n",
            "Learning\n",
            "Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e]\n",
            "\n",
            "Metadata:\n",
            "  id: 38\n",
            "  language: en\n",
            "  source: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "  title: Artificial intelligence - Wikipedia\n",
            "--------------------------------------------------\n",
            "Document 2:\n",
            "Content:\n",
            "No established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI\n",
            "\n",
            "Metadata:\n",
            "  id: 254\n",
            "  language: en\n",
            "  source: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "  title: Artificial intelligence - Wikipedia\n",
            "--------------------------------------------------\n",
            "Document 3:\n",
            "Content:\n",
            "There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45]\n",
            "\n",
            "Metadata:\n",
            "  id: 39\n",
            "  language: en\n",
            "  source: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "  title: Artificial intelligence - Wikipedia\n",
            "--------------------------------------------------\n",
            "Document 4:\n",
            "Content:\n",
            "^ a b Supervised learning: Russell & Norvig (2021, §19.2) (Definition), Russell & Norvig (2021, Chpt. 19–20) (Techniques)\n",
            "\n",
            "^ Reinforcement learning: Russell & Norvig (2021, chpt. 22), Luger & Stubblefield (2004, pp. 442–449)\n",
            "\n",
            "^ Transfer learning: Russell & Norvig (2021, pp. 281), The Economist (2016)\n",
            "\n",
            "^ \"Artificial Intelligence (AI): What Is AI and How Does It Work? | Built In\". builtin.com. Retrieved 30 October 2023.\n",
            "\n",
            "Metadata:\n",
            "  id: 327\n",
            "  language: en\n",
            "  source: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "  title: Artificial intelligence - Wikipedia\n",
            "--------------------------------------------------\n",
            "Document 5:\n",
            "Content:\n",
            "McCarthy, John (2007), \"From Here to Human-Level AI\", Artificial Intelligence, p. 171\n",
            "McCarthy, John (1999), What is AI?, archived from the original on 4 December 2022, retrieved 4 December 2022\n",
            "McCauley, Lee (2007). \"AI armageddon and the three laws of robotics\". Ethics and Information Technology. 9 (2): 153–164. CiteSeerX 10.1.1.85.8904. doi:10.1007/s10676-007-9138-2. S2CID 37272949.\n",
            "\n",
            "Metadata:\n",
            "  id: 471\n",
            "  language: en\n",
            "  source: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "  title: Artificial intelligence - Wikipedia\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}